{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to calculate group differences based on paper drakesmith et al\n",
    "\n",
    "This is the  multi-threshold permutation correction approach and consists:\n",
    "\n",
    "1) Apply thresholds to networks and compute network metrics for all networks across all thresholds\n",
    "2) Compute test statistics for each network\n",
    "3) Permute across groups to get the null distrubition\n",
    "4) Take the maximum test statistic across all thresholds for each permutation resulting in one summarised null statistic for each permutation. \n",
    "5) Identify the critical value for the test-statistic from the top Î±th percentile of the null test statistics, where Î± is the desired confidence level (e.g., 5%).\n",
    "6) Identify clusters where the true test statistic is higher than the critical value and compute the AUC for these clusters.\n",
    "7) Compute a critical AUC from the mean of the super-critical AUCs for the permuted tests (denoted Acrit).\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import functions.plotting_functions as Pfun\n",
    "import functions.statistical_functions as Sfun\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "from decouple import config\n",
    "\n",
    "data = config('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pval_plotting(ctvalues:list, tcrit_value:int, measure:str, permutations:int, crit_val:int) -> None:\n",
    "    \n",
    "    '''\n",
    "    Wrapper around seaborn displot.\n",
    "    Function to plot null distribution with 2 std either side and true critical value.\n",
    "\n",
    "    Parameters\n",
    "    -----------------------------------------------------------------\n",
    "    ctvalues : list, list of crticial values from the null distrubtion.\n",
    "    tcrit_value: int True crtical value of a test.\n",
    "    measure: str, name of the measure (used in title of the graph)\n",
    "    permutations: int, number of permuations (used in title of the graph)\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -----------------------------------------------------------------\n",
    "    hist: histogram of null distribution with upper and lower std with\n",
    "          true value.\n",
    "    '''\n",
    "    \n",
    "    #Calculates the mean and std deviation of null distribution\n",
    "\n",
    "    mean_val = Sfun.mean_std(ctvalues)\n",
    "    \n",
    "    #plots \n",
    "    hist = sns.displot(data=ctvalues, height=10).set(title=f'Histplot for {measure} after {permutations} permuatations')\n",
    "    hist.refline(x=tcrit_value, color='purple')\n",
    "    hist.refline(x=crit_val, color='green', linestyle='-')\n",
    "    hist.refline(x=mean_val['lower_2std'], linestyle='-', color='black')\n",
    "    hist.refline(x=mean_val['upper_2std'], linestyle='-', color='black')\n",
    "    hist.refline(x=mean_val['lower_1std'], linestyle='-', color='red')\n",
    "    hist.refline(x=mean_val['upper_1std'], linestyle='-', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lh_volume = pd.read_csv(f'{data}/lh_volume.dat',sep='\\t').drop(['BrainSegVolNotVent', 'eTIV'],axis=1).rename(columns={'lh.aparc.volume':'G-Number'})\n",
    "rh_volume =  pd.read_csv(f'{data}/rh_volume.dat',sep='\\t').drop(['BrainSegVolNotVent', 'eTIV','rh.aparc.volume'],axis=1)\n",
    "\n",
    "group = pd.read_csv(f'{data}/cortical_measures.csv').iloc[0:,2]\n",
    "\n",
    "volume = pd.concat([lh_volume, rh_volume, group],axis=1)\n",
    "\n",
    "names = list(volume.columns.drop(['G-Number','age_adjusted_group']))\n",
    "\n",
    "centroids = pd.read_csv(f'{data}/atlas.csv')\n",
    "centroids = centroids[['x.mni', 'y.mni', 'z.mni']].to_numpy()\n",
    "\n",
    "\n",
    "group = volume.groupby('age_adjusted_group')\n",
    "aan = group.get_group('AAN').reset_index(drop=True)\n",
    "hc = group.get_group('HC').reset_index(drop=True)\n",
    "wr = group.get_group('WR').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Apply thresholds to networks and compute network metrics for all networks across all thresholds\n",
    "\n",
    "First the threshold range needs to be decided. To do this the methodology used is taken from Bassett et al (https://doi.org/10.1073/pnas.0606005103) where the upper limit must not exceed 2 * natural log(nodes). \n",
    "Starting point of 4 is used otherwise minimum spanning tree is too large and throws up error message.\n",
    "\n",
    "Threshold values are stored in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_value=[]\n",
    "for threshold in range(4 , 100):\n",
    "    aan_graphs = Sfun.create_graphs(aan.iloc[:,1:69], names, centroids, threshold=threshold)\n",
    "    aan_graphs['graph_threshold'].calculate_nodal_measures()\n",
    "    cal = aan_graphs['graph_threshold'].report_nodal_measures()\n",
    "    \n",
    "    if cal['degree'].mean() > 2 * np.log(len(aan_graphs['graph_threshold'].nodes())):\n",
    "        break\n",
    "    \n",
    "    threshold_value.append(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create graphs at the pre-defined thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'aan_graphs':[],\n",
    "    'hc_graphs':[],\n",
    "    'wr_graphs':[]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in threshold_value:\n",
    "    aan_graphs = Sfun.create_graphs(aan.iloc[:,1:69], names, centroids, threshold=threshold)\n",
    "    aan_graphs['graph_threshold'].__name__ = f'aan_graph_threshold_value_{threshold}'\n",
    "    result['aan_graphs'].append(aan_graphs['graph_threshold'])\n",
    "\n",
    "    hc_graphs = Sfun.create_graphs(hc.iloc[:,1:69], names, centroids, threshold=threshold)\n",
    "    hc_graphs['graph_threshold'].__name__ = f'hc_graph_threshold_value_{threshold}'\n",
    "    result['hc_graphs'].append(hc_graphs['graph_threshold'])\n",
    "\n",
    "    wr_graphs = Sfun.create_graphs(wr.iloc[:,1:69], names, centroids, threshold=threshold)\n",
    "    wr_graphs['graph_threshold'].__name__ = f'wr_graph_threshold_value_{threshold}'\n",
    "    result['wr_graphs'].append(wr_graphs['graph_threshold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally calculate global measures for each graph at the threshold ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keys, values in result.items():\n",
    "    for graph_object in values:\n",
    "        global_measures = graph_object.calculate_global_measures()\n",
    "        measures[f'{graph_object.__name__}'] = []\n",
    "        measures[f'{graph_object.__name__}'].append(global_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Compute test statistics for each network\n",
    "\n",
    "Calculate the test statistic for each condition. This is going to be the difference between each global measure for two groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_statistics = {\n",
    "    'aan_hc':{},\n",
    "    'wr_hc':{},\n",
    "    'wr_aan':{}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in threshold_value:\n",
    "    for keys, value in measures['aan_graph_threshold_value_4'][0].items():\n",
    "        statistic_aan_hc =  measures[f'aan_graph_threshold_value_{threshold}'][0][keys] - measures[f'hc_graph_threshold_value_{threshold}'][0][keys]\n",
    "        statistic_wr_hc =  measures[f'wr_graph_threshold_value_{threshold}'][0][keys] - measures[f'hc_graph_threshold_value_{threshold}'][0][keys]\n",
    "        statistic_wr_aan =  measures[f'wr_graph_threshold_value_{threshold}'][0][keys] - measures[f'aan_graph_threshold_value_{threshold}'][0][keys]\n",
    "        \n",
    "        key = f'{keys}_at_threshold_value_{threshold}'\n",
    "      \n",
    "        test_statistics['aan_hc'][key] = statistic_aan_hc \n",
    "        test_statistics['wr_hc'][key] = statistic_wr_hc\n",
    "        test_statistics['wr_aan'][key] = statistic_wr_aan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Permute across groups to get the null distrubition\n",
    "\n",
    "Now we permutate to create a null distrubtion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_measures() -> list:\n",
    "    '''\n",
    "    Function to get list of graph measures\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list: list of graph theory measures\n",
    "    '''\n",
    "    return ['average_clustering', 'average_shortest_path_length', 'assortativity', 'modularity', 'efficiency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permuations(permutations:int, length_of_group_1:int, length_of_group_2:int, participants:pd.DataFrame, names:list, centroids:np.float64, threshold:int) -> dict:\n",
    "    measures = list_of_measures()\n",
    "    null_distribution ={\n",
    "        \n",
    "        'average_clustering':[],\n",
    "        'average_shortest_path_length':[], \n",
    "        'assortativity':[], \n",
    "        'modularity':[], \n",
    "        'efficiency':[]\n",
    "    \n",
    "    }\n",
    "    \n",
    "    for perm in range(permutations):\n",
    "        group_1_participants = participants.sample(n=length_of_group_1)\n",
    "        group_2_participants = participants.sample(n=length_of_group_2)\n",
    "        \n",
    "        group_1_graphs = Sfun.create_graphs(group_1_participants.iloc[:,1:69], names, centroids, threshold=threshold)\n",
    "        group_2_graphs = Sfun.create_graphs(group_2_participants.iloc[:,1:69], names, centroids)\n",
    "        \n",
    "        group_1_values = group_1_graphs['graph_threshold'].calculate_global_measures()\n",
    "        group_2_graphs = group_2_graphs['graph_threshold'].calculate_global_measures()\n",
    "    \n",
    "        for meas in measures:\n",
    "            crit_val = group_1_values[meas] -  group_2_graphs[meas]\n",
    "            null_distribution[meas].append(crit_val)\n",
    "            \n",
    "    return null_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aan_hc_df = pd.concat([aan, hc], ignore_index=True)\n",
    "aan_wr_df = pd.concat([wr, aan], ignore_index=True)\n",
    "wr_hc_df = pd.concat([wr, hc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null_distribution = {\n",
    "    'aan_hc':{},\n",
    "    'wr_hc':{},\n",
    "    'wr_aan':{}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for threshold in threshold_value:\n",
    "    aan_hc_perm = permuations(100, len(aan['G-Number']), len(hc['G-Number']), aan_hc_df, names, centroids, threshold)\n",
    "    aan_wr_perm = permuations(100, len(aan['G-Number']), len(wr['G-Number']), aan_wr_df, names, centroids, threshold)\n",
    "    wr_hc_perm = permuations(100, len(wr['G-Number']), len(hc['G-Number']), wr_hc_df, names, centroids, threshold)\n",
    "    \n",
    "    key = f'thresholded_value_{threshold}'\n",
    "    null_distribution['aan_hc'][key] = aan_hc_perm \n",
    "    null_distribution['wr_hc'][key] = aan_wr_perm\n",
    "    null_distribution['wr_aan'][key] = wr_hc_perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('permutations.pickle', 'wb') as handle:\n",
    "    pickle.dump(null_distribution, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('permutations.pickle', 'rb') as handle:\n",
    "    null_distribution = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.231782265144864\n",
      "-0.08077260755048243\n"
     ]
    }
   ],
   "source": [
    "print(max(null_distribution['aan_hc']['thresholded_value_4']['average_shortest_path_length']))\n",
    "print(test_statistics['aan_hc']['average_shortest_path_length_at_threshold_value_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Take the maximum test statistic across all thresholds for each permutation resulting in one summarised null statistic for each permutation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_null_stat(null_distribution:dict, group_key:str, measure_key:str, list_number:int) -> dict:\n",
    "    \n",
    "    '''\n",
    "    Function to loop through each element in a list for each thresold value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    null_distribution:dict dictionary of null_distribution\n",
    "    group_key:str dictionary key for group \n",
    "    measure_key:str dictionary key for the graph theory measure.\n",
    "    list_number:int list index of permutation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    max_null_statistic:int max null statistic for that permutation.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    values = []\n",
    "    for threshold in threshold_value:\n",
    "        values.append(null_distribution[group_key][f'thresholded_value_{threshold}'][measure_key][list_number])\n",
    "        \n",
    "    max_null_statistic = max(values, key=abs)\n",
    "\n",
    "    max_null_dict = {\n",
    "        'summarised_values':values,\n",
    "        'max_null':max_null_statistic\n",
    "    }\n",
    "    \n",
    "    return max_null_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_stat_summarized = {\n",
    "    \n",
    "    'aan_hc':{},\n",
    "    'wr_hc':{},\n",
    "    'wr_aan':{}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once dictionary has been initialised it needs to be set up to be able to take the permutations and list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = list_of_measures()\n",
    "for group_key in null_stat_summarized.keys():\n",
    "    for measure in measures:\n",
    "        null_stat_summarized[group_key][measure] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_key in null_stat_summarized.keys():\n",
    "    for perm in range(0, 100):\n",
    "        for measure in measures:\n",
    "            max_null_statistic = find_max_null_stat(null_distribution, group_key, measure, perm )\n",
    "            null_stat_summarized[group_key][measure].append(max_null_statistic['max_null'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Identify the critical value for the test-statistic from the top Î±th percentile of the null test statistics, where Î± is the desired confidence level (e.g., 5%).\n",
    "\n",
    "To take into account direction of effect the Max value is calculated at both ends of the distribution.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_val = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for top_key in null_stat_summarized.keys():\n",
    "    for measure in measures:\n",
    "        crit_val[f'{top_key}_{measure}']= [np.quantile(np.array(null_stat_summarized[top_key][measure]), 0.05), \n",
    "                                           np.quantile(np.array(null_stat_summarized[top_key][measure]), 0.95)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Identify clusters where the true test statistic is higher than the critical value and compute the AUC for these clusters.\n",
    "\n",
    "This is done by checking that the statistic is greater than the max value on both ends of the distribution.\n",
    "\n",
    "First work out clusters greater than the max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "\n",
    "for top_key in test_statistics.keys():\n",
    "    for key , val in test_statistics[top_key].items():\n",
    "        new_key = re.sub(r'_at_threshold_value_.*','', key)\n",
    "        critical_value = crit_val[f'{top_key}_{new_key}']\n",
    "\n",
    "        lower = val < critical_value[0]\n",
    "        upper =  val > critical_value[1]\n",
    "        direction = lambda lower, upper: 'lower' if lower ==True else 'upper'\n",
    "\n",
    "        if lower or upper == True:\n",
    "            clusters[f'{top_key}_{key}'] = {'direction':direction(lower, upper), \n",
    "                                            'value':test_statistics[top_key][key], \n",
    "                                            'null':null_stat_summarized[top_key][new_key]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = {}\n",
    "\n",
    "for measure_key in clusters.keys():\n",
    "    if clusters[measure_key]['direction'] == 'upper':\n",
    "        sum_list = [val for val in clusters[measure_key]['null'] if val >= clusters[measure_key]['value']]\n",
    "        pvals[measure_key] = (len(sum_list)/100) \n",
    "        \n",
    "    if clusters[measure_key]['direction'] == 'lower':\n",
    "        sum_list = [val for val in clusters[measure_key]['null'] if val <= clusters[measure_key]['value']]\n",
    "        pvals[measure_key] = (len(sum_list)/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a dictionary with all the threshold values in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dic = {}\n",
    "\n",
    "for group_key in test_statistics.keys():\n",
    "    for measure in measures:\n",
    "        check_dic[f'{group_key}_{measure}'] = []\n",
    "        \n",
    "        for key, val in clusters.items():\n",
    "            if measure in key:\n",
    "                if group_key in key:\n",
    "                    check_dic[f'{group_key}_{measure}'].append(clusters[key]['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is taken from leons code. I think it is creating a high spec grid (matrix) then linear interploating it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = {\n",
    "    'aan_hc':{},\n",
    "    'wr_hc':{},\n",
    "    'wr_aan':{}\n",
    "\n",
    "}\n",
    "for group_key in auc.keys():\n",
    "    for measure in measures:\n",
    "        if len(check_dic[f'{group_key}_{measure}']) == len(threshold_value):\n",
    "            threshold_values_linspace = np.linspace(start=min(threshold_value), stop=max(threshold_value), num=500)\n",
    "            observation_interploated = np.interp(threshold_values_linspace, threshold_value, check_dic[f'{group_key}_{measure}'])\n",
    "            obs_AUC = np.trapz(threshold_values_linspace, observation_interploated) \n",
    "            auc[group_key][measure] = obs_AUC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Compute a critical AUC from the mean of the super-critical AUCs for the permuted tests (denoted Acrit).\n",
    "\n",
    "This does exactly the same as for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_permutations = {\n",
    "    \n",
    "    'aan_hc':{},\n",
    "    'wr_hc':{},\n",
    "    'wr_aan':{}\n",
    "}\n",
    "\n",
    "for group_key in summarized_permutations.keys():\n",
    "    for measure in measures:\n",
    "        summarized_permutations[group_key][measure] = []\n",
    "\n",
    "for group_key in summarized_permutations.keys():\n",
    "    for perm in range(0, 100):\n",
    "        for measure in measures:\n",
    "            max_null_statistic_values = find_max_null_stat(null_distribution, group_key, measure, perm )\n",
    "            summarized_permutations[group_key][measure].append(max_null_statistic_values['summarised_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_auc ={\n",
    "    'aan_hc':{},\n",
    "    'wr_hc':{},\n",
    "    'wr_aan':{}\n",
    "}\n",
    "\n",
    "for group_key in auc.keys():\n",
    "    for measure in measures:\n",
    "        null_auc[group_key][measure] = []\n",
    "        for perm in range(0, 100):\n",
    "            if len(check_dic[f'{group_key}_{measure}']) == len(threshold_value):\n",
    "                threshold_values_linspace = np.linspace(start=min(threshold_value), stop=max(threshold_value), num=500)\n",
    "                observation_interploated = np.interp(threshold_values_linspace, threshold_value, summarized_permutations[group_key][measure][perm])\n",
    "                obs_AUC = np.trapz(threshold_values_linspace, observation_interploated) \n",
    "                null_auc[group_key][measure].append(obs_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7933250005253527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.07650247096045792"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(null_auc['aan_hc']['average_clustering'])/100)\n",
    "auc['aan_hc']['average_clustering']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6aaeafbb34bd8ccb4b94759bad6d8a43f22b1f97d06ebd89ca25d302d210f79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
